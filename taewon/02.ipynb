{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled59.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOJLHdxQ5BP6xrLtGCl1+3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t1seo/AIFFEL_Hackerthon-1/blob/main/taewon/02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIclL_0fMpJj"
      },
      "source": [
        "- [Simple lightgbm LB:0.493](https://www.kaggle.com/festa78/simple-lightgbm-lb-0-493)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKFh4t2aMcTd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WYIBINWMiPH"
      },
      "source": [
        "cd /content/drive/MyDrive/data/Restaurant_Visitor_Forecasting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCg54syDMQUY"
      },
      "source": [
        "\"\"\"\n",
        "Ref: https://www.kaggle.com/the1owl/surprise-me\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Data wrangling brought to you by the1owl\n",
        "# https://www.kaggle.com/the1owl/surprise-me\n",
        "\n",
        "data = {\n",
        "    'tra':\n",
        "    pd.read_csv('air_visit_data.csv'),\n",
        "    'as':\n",
        "    pd.read_csv('air_store_info.csv'),\n",
        "    'hs':\n",
        "    pd.read_csv('hpg_store_info.csv'),\n",
        "    'ar':\n",
        "    pd.read_csv('air_reserve.csv'),\n",
        "    'hr':\n",
        "    pd.read_csv('hpg_reserve.csv'),\n",
        "    'id':\n",
        "    pd.read_csv('store_id_relation.csv'),\n",
        "    'tes':\n",
        "    pd.read_csv('sample_submission.csv'),\n",
        "    'hol':\n",
        "    pd.read_csv('date_info.csv').rename(columns={\n",
        "        'calendar_date': 'visit_date'\n",
        "    })\n",
        "}\n",
        "\n",
        "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
        "\n",
        "for df in ['ar', 'hr']:\n",
        "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
        "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
        "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
        "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
        "    data[df]['reserve_datetime_diff'] = data[df].apply(\n",
        "        lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
        "    data[df] = data[df].groupby(\n",
        "        ['air_store_id', 'visit_datetime'], as_index=False)[[\n",
        "            'reserve_datetime_diff', 'reserve_visitors'\n",
        "        ]].sum().rename(columns={\n",
        "            'visit_datetime': 'visit_date'\n",
        "        })\n",
        "    print(data[df].head())\n",
        "\n",
        "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
        "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
        "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
        "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
        "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
        "\n",
        "data['tes']['visit_date'] = data['tes']['id'].map(\n",
        "    lambda x: str(x).split('_')[2])\n",
        "data['tes']['air_store_id'] = data['tes']['id'].map(\n",
        "    lambda x: '_'.join(x.split('_')[:2]))\n",
        "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
        "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
        "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
        "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
        "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
        "\n",
        "unique_stores = data['tes']['air_store_id'].unique()\n",
        "stores = pd.concat(\n",
        "    [\n",
        "        pd.DataFrame({\n",
        "            'air_store_id': unique_stores,\n",
        "            'dow': [i] * len(unique_stores)\n",
        "        }) for i in range(7)\n",
        "    ],\n",
        "    axis=0,\n",
        "    ignore_index=True).reset_index(drop=True)\n",
        "\n",
        "#sure it can be compressed...\n",
        "tmp = data['tra'].groupby(\n",
        "    ['air_store_id', 'dow'],\n",
        "    as_index=False)['visitors'].min().rename(columns={\n",
        "        'visitors': 'min_visitors'\n",
        "    })\n",
        "stores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\n",
        "tmp = data['tra'].groupby(\n",
        "    ['air_store_id', 'dow'],\n",
        "    as_index=False)['visitors'].mean().rename(columns={\n",
        "        'visitors': 'mean_visitors'\n",
        "    })\n",
        "stores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\n",
        "tmp = data['tra'].groupby(\n",
        "    ['air_store_id', 'dow'],\n",
        "    as_index=False)['visitors'].median().rename(columns={\n",
        "        'visitors': 'median_visitors'\n",
        "    })\n",
        "stores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\n",
        "tmp = data['tra'].groupby(\n",
        "    ['air_store_id', 'dow'],\n",
        "    as_index=False)['visitors'].max().rename(columns={\n",
        "        'visitors': 'max_visitors'\n",
        "    })\n",
        "stores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\n",
        "tmp = data['tra'].groupby(\n",
        "    ['air_store_id', 'dow'],\n",
        "    as_index=False)['visitors'].count().rename(columns={\n",
        "        'visitors': 'count_observations'\n",
        "    })\n",
        "stores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\n",
        "\n",
        "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id'])\n",
        "lbl = preprocessing.LabelEncoder()\n",
        "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
        "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
        "\n",
        "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
        "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
        "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
        "\n",
        "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date'])\n",
        "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date'])\n",
        "\n",
        "train = pd.merge(data['tra'], stores, how='left', on=['air_store_id', 'dow'])\n",
        "test = pd.merge(data['tes'], stores, how='left', on=['air_store_id', 'dow'])\n",
        "\n",
        "for df in ['ar', 'hr']:\n",
        "    train = pd.merge(\n",
        "        train, data[df], how='left', on=['air_store_id', 'visit_date'])\n",
        "    test = pd.merge(\n",
        "        test, data[df], how='left', on=['air_store_id', 'visit_date'])\n",
        "\n",
        "col = [\n",
        "    c for c in train\n",
        "    if c not in ['id', 'air_store_id', 'visit_date', 'visitors']\n",
        "]\n",
        "train = train.fillna(-1)\n",
        "test = test.fillna(-1)\n",
        "\n",
        "print('Binding to float32')\n",
        "\n",
        "for c, dtype in zip(train.columns, train.dtypes):\n",
        "    if dtype == np.float64:\n",
        "        train[c] = train[c].astype(np.float32)\n",
        "\n",
        "for c, dtype in zip(test.columns, test.dtypes):\n",
        "    if dtype == np.float64:\n",
        "        test[c] = test[c].astype(np.float32)\n",
        "\n",
        "train_x = train.drop(['air_store_id', 'visit_date', 'visitors'], axis=1)\n",
        "train_y = np.log1p(train['visitors'].values)\n",
        "print(train_x.shape, train_y.shape)\n",
        "test_x = test.drop(['id', 'air_store_id', 'visit_date', 'visitors'], axis=1)\n",
        "\n",
        "# parameter tuning of lightgbm\n",
        "# start from default setting\n",
        "gbm0 = lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    num_leaves=60,\n",
        "    learning_rate=0.01,\n",
        "    n_estimators=10000)\n",
        "\n",
        "gbm0.fit(train_x, train_y, eval_metric='rmse')\n",
        "predict_y = gbm0.predict(test_x)\n",
        "test['visitors'] = np.expm1(predict_y)\n",
        "test[['id', 'visitors']].to_csv(\n",
        "    'gbm0_submission.csv', index=False, float_format='%.3f')  # LB0.493"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51wtZY1ZMTgf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}