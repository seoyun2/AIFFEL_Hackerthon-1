{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled58.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOKYoryuvGI3gePI4tsjItu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t1seo/AIFFEL_Hackerthon-1/blob/main/taewon/01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2sDcHrwE_lD"
      },
      "source": [
        "## 간단해 보이는 커널 분석\n",
        "\n",
        "- [Surprised Yet? - Part2 - (LB: 0.503)](https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503)\n",
        "- [Submission_0.483](https://www.kaggle.com/bibinpaul/submission-0-483)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVAcw_QwFIrR",
        "outputId": "9eca2f77-89d3-45af-b2a9-f127db8b3ece"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKDERKrpFN39",
        "outputId": "90742b0e-0536-4be8-9729-f267eb20744d"
      },
      "source": [
        "cd /content/drive/MyDrive/data/Restaurant_Visitor_Forecasting"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/data/Restaurant_Visitor_Forecasting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be6PvGZYFV4C",
        "outputId": "c2c68f75-4045-4606-9ebc-ea18fa6f1e5c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import ensemble, neighbors, linear_model, metrics, preprocessing\n",
        "from datetime import datetime\n",
        "\n",
        "data = {\n",
        "    'tra': pd.read_csv('air_visit_data.csv'),\n",
        "    'as': pd.read_csv('air_store_info.csv'),\n",
        "    'hs': pd.read_csv('hpg_store_info.csv'),\n",
        "    'ar': pd.read_csv('air_reserve.csv'),\n",
        "    'hr': pd.read_csv('hpg_reserve.csv'),\n",
        "    'id': pd.read_csv('store_id_relation.csv'),\n",
        "    'tes': pd.read_csv('sample_submission.csv'),\n",
        "    'hol': pd.read_csv('date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
        "    }\n",
        "\n",
        "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
        "\n",
        "for df in ['ar','hr']:\n",
        "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
        "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
        "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
        "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
        "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
        "    data[df] = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date'})\n",
        "    print(data[df].head())\n",
        "\n",
        "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
        "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
        "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
        "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
        "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
        "\n",
        "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
        "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
        "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
        "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
        "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
        "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
        "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
        "\n",
        "unique_stores = data['tes']['air_store_id'].unique()\n",
        "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
        "\n",
        "#sure it can be compressed...\n",
        "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
        "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
        "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
        "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
        "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
        "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
        "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
        "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
        "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
        "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
        "\n",
        "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
        "lbl = preprocessing.LabelEncoder()\n",
        "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
        "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
        "\n",
        "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
        "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
        "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
        "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
        "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
        "\n",
        "train = pd.merge(data['tra'], stores, how='left', on=['air_store_id','dow']) \n",
        "test = pd.merge(data['tes'], stores, how='left', on=['air_store_id','dow'])\n",
        "\n",
        "for df in ['ar','hr']:\n",
        "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
        "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
        "    \n",
        "print(train.describe())\n",
        "print(train.head())\n",
        "\n",
        "col = [c for c in train if c not in ['id', 'air_store_id','visit_date','visitors']]\n",
        "train = train.fillna(-1)\n",
        "test = test.fillna(-1)\n",
        "\n",
        "def RMSLE(y, pred):\n",
        "    return metrics.mean_squared_error(y, pred)**0.5\n",
        "\n",
        "#lr = linear_model.LinearRegression(n_jobs=-1)\n",
        "etc = ensemble.ExtraTreesRegressor(n_estimators=225, max_depth=5, n_jobs=-1, random_state=3)\n",
        "knn = neighbors.KNeighborsRegressor(n_jobs=-1, n_neighbors=4)\n",
        "#lr.fit(train[col], np.log1p(train['visitors'].values))\n",
        "etc.fit(train[col], np.log1p(train['visitors'].values))\n",
        "knn.fit(train[col], np.log1p(train['visitors'].values))\n",
        "#print('RMSE LinearRegressor: ', RMSLE(np.log1p(train['visitors'].values), lr.predict(train[col])))\n",
        "print('RMSE ExtraTreesRegressor: ', RMSLE(np.log1p(train['visitors'].values), etc.predict(train[col])))\n",
        "print('RMSE KNNRegressor: ', RMSLE(np.log1p(train['visitors'].values), knn.predict(train[col])))\n",
        "\n",
        "test['visitors'] = (etc.predict(test[col]) / 2) +(knn.predict(test[col]) / 2)\n",
        "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
        "test[['id','visitors']].to_csv('lr_submission.csv', index=False, float_format='%.2f')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           air_store_id  visit_date  reserve_datetime_diff  reserve_visitors\n",
            "0  air_00a91d42b08b08d9  2016-10-31                      0                 2\n",
            "1  air_00a91d42b08b08d9  2016-12-05                      4                 9\n",
            "2  air_00a91d42b08b08d9  2016-12-14                      6                18\n",
            "3  air_00a91d42b08b08d9  2016-12-17                      6                 2\n",
            "4  air_00a91d42b08b08d9  2016-12-20                      2                 4\n",
            "           air_store_id  visit_date  reserve_datetime_diff  reserve_visitors\n",
            "0  air_00a91d42b08b08d9  2016-01-14                      3                 2\n",
            "1  air_00a91d42b08b08d9  2016-01-15                      6                 4\n",
            "2  air_00a91d42b08b08d9  2016-01-16                      3                 2\n",
            "3  air_00a91d42b08b08d9  2016-01-22                      3                 2\n",
            "4  air_00a91d42b08b08d9  2016-01-29                      6                 5\n",
            "            visitors  ...  reserve_visitors_y\n",
            "count  252108.000000  ...        13550.000000\n",
            "mean       20.973761  ...            6.803469\n",
            "std        16.757007  ...            7.686077\n",
            "min         1.000000  ...            1.000000\n",
            "25%         9.000000  ...            2.000000\n",
            "50%        17.000000  ...            4.000000\n",
            "75%        29.000000  ...            8.000000\n",
            "max       877.000000  ...          157.000000\n",
            "\n",
            "[8 rows x 17 columns]\n",
            "           air_store_id  ... reserve_visitors_y\n",
            "0  air_ba937bf13d40fb24  ...                NaN\n",
            "1  air_ba937bf13d40fb24  ...                NaN\n",
            "2  air_ba937bf13d40fb24  ...                NaN\n",
            "3  air_ba937bf13d40fb24  ...                NaN\n",
            "4  air_ba937bf13d40fb24  ...                NaN\n",
            "\n",
            "[5 rows x 19 columns]\n",
            "RMSE ExtraTreesRegressor:  0.5238810527950593\n",
            "RMSE KNNRegressor:  0.4255759709458109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9Bz53k3FXtw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}